# LLM-Mistral-Fine-tuning-Zephyr-Chatbot-Deployment-GenAI
Designed the full training pipeline using HuggingFace transformers and datasets and implemented LoRA adapters for efficient finetuning

Finetuning Mistral-7B using QLoRA for memory-efficient performance, and deploying a Zephyr-based chatbot with conversation memory and fast inference.

 Tech Stack: Python 路 HuggingFace Transformers 路 QLoRA 路 Mistral-7B 路 Zephyr 路 LoRA 路 BitsAndBytes

 Features:

4-bit quantized training pipeline using QLoRA + LoRA adapters

Trained on 1.5k+ prompt-response pairs for conversational modeling

Deployed Zephyr chatbot with memory and <700ms latency (manual benchmark)

Custom model loading, prompt formatting, and memory handling

Ideal Usage: Demonstrates fine-tuning for lightweight LLM deployment with fast inference and retention.
